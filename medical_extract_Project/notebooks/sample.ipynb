{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84be1c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt has hx of dm and htn prescribed metformin 500mg bid followup in 3 weeks\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load a sample note\n",
    "note = \"\"\"Pt has hx of DM and HTN. Prescribed Metformin 500mg BID. Follow-up in 3 weeks.\"\"\"\n",
    "\n",
    "# Basic cleanup\n",
    "note = note.lower()\n",
    "note = re.sub(r'[^\\w\\s]', '', note)\n",
    "\n",
    "print(note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f5d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!', 'Welcome to this tutorial on tokenizing.', 'After going through this tutorial you will be able to tokenize your text.', 'Tokenizing is an important concept under NLP.', 'Happy learning!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab', force=True)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Hello there! Welcome to this tutorial on tokenizing. After going through this tutorial you will be able to tokenize your text. Tokenizing is an important concept under NLP. Happy learning!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fd6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'Welcome', 'to', 'this', 'tutorial', 'on', 'tokenizing', '.', 'After', 'going', 'through', 'this', 'tutorial', 'you', 'will', 'be', 'able', 'to', 'tokenize', 'your', 'text', '.', 'Tokenizing', 'is', 'an', 'important', 'concept', 'under', 'NLP', '.', 'Happy', 'learning', '!']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "473f6a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there!', 'Welcome', 'to', 'this', 'tutorial', 'on', 'tokenizing.', 'After', 'going', 'through', 'this', 'tutorial', 'you', 'will', 'be', 'able', 'to', 'tokenize', 'your', 'text.', 'Tokenizing', 'is', 'an', 'important', 'concept', 'under', 'NLP.', 'Happy', 'learning!']\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc830a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 25.1.1 from C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pip (python 3.11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip --version scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas scikit-learn spacy scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c777ad9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'umls_ents'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe patient was diagnosed with hypertension and prescribed amlodipine.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ent\u001b[38;5;241m.\u001b[39mtext, ent\u001b[38;5;241m.\u001b[39mlabel_, \u001b[43ment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumls_ents\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\tokens\\underscore.py:47\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     48\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'umls_ents'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "linker = EntityLinker(resolve_abbreviations=True, name=\"umls\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "doc = nlp(\"The patient was diagnosed with hypertension and prescribed amlodipine.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent._.umls_ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0881eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you love me!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"you love me!\")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "\n",
    "nlp =  spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31469f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1544027337.000000</th>\n",
       "      <th>1544027337.000000</th>\n",
       "      <th>1544027337.000000.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748682</th>\n",
       "      <td>-24.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748683</th>\n",
       "      <td>-24.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748684</th>\n",
       "      <td>-25.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748685</th>\n",
       "      <td>-24.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748686</th>\n",
       "      <td>-24.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748687 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1544027337.000000   1544027337.000000   1544027337.000000.1\n",
       "0                    32.0                32.0                  32.0\n",
       "1                    -3.0                65.0                   6.0\n",
       "2                    -3.0                65.0                   6.0\n",
       "3                    -3.0                65.0                   6.0\n",
       "4                    -3.0                65.0                   6.0\n",
       "...                   ...                 ...                   ...\n",
       "748682              -24.0                42.0                  42.0\n",
       "748683              -24.0                42.0                  43.0\n",
       "748684              -25.0                42.0                  42.0\n",
       "748685              -24.0                42.0                  44.0\n",
       "748686              -24.0                43.0                  42.0\n",
       "\n",
       "[748687 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataame = pd.read_csv(\"C:/Users/User/Desktop/formation/Formation python/medical_extract_Project/data/Data/Data/S1/Final/ACC.csv\")\n",
    "dataame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee332692",
   "metadata": {},
   "source": [
    "### Let’s Try spaCy’s Rule-Based Matcher\n",
    "\n",
    "Let’s try **spaCy’s rule-based Matcher**. You’ll be using the example from the previous exercise and writing a pattern that can match the phrase **“iPhone X”** in the text.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Import the `Matcher` from `spacy.matcher`.\n",
    "2. Initialize it with the `nlp` object’s shared vocab.\n",
    "3. Create a pattern that matches the `\"TEXT\"` values of two tokens: `\"iPhone\"` and `\"X\"`.\n",
    "4. Use the `matcher.add()` method to add the pattern to the matcher.\n",
    "5. Call the matcher on the `doc` and store the result in the variable `matches`.\n",
    "6. Iterate over the matches and get the matched span from the start to the end index.\n",
    "\n",
    "\n",
    "Example\tDescription\n",
    " - {\"OP\": \"!\"}\tNegation: match 0 times\n",
    " - {\"OP\": \"?\"}\tOptional: match 0 or 1 times\n",
    " - {\"OP\": \"+\"}\tMatch 1 or more times\n",
    " - {\"OP\": \"*\"}\tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74307728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76d2d9",
   "metadata": {},
   "source": [
    "#### **Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3d2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total macthes found: 3\n",
      "Macth found iOS 7\n",
      "Macth found iOS 11\n",
      "Macth found iOS 10\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total macthes found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for macthe_id , start, end in matches:\n",
    "    print(\"Macth found\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c35999",
   "metadata": {},
   "source": [
    "#### **Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0836aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5333c",
   "metadata": {},
   "source": [
    "#### **Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1dc025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b873f4",
   "metadata": {},
   "source": [
    "##### **Import the Doc from spacy.tokens.</br>Create a Doc from the words and spaces. Don’t forget to pass in the vocab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90381ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n",
      "Go, get started!\n",
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10911a",
   "metadata": {},
   "source": [
    "In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
    "\n",
    " - Import the Doc and Span classes from spacy.tokens.\n",
    " - Use the Doc class directly to create a doc from the words and spaces.\n",
    " - Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    " - Overwrite the doc.ents with a list of one entity, the “David Bowie” span.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da01cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f977d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n",
      "Found proper noun before a verb: moscow\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city. and moscow leads the parad\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119eaedf",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "*Rewrite the code to use the native token attributes instead of lists of and .token_textspos_tags </br>\n",
    "Loop over each in the and check the attribute.tokendoctoken.pos_</br>\n",
    "Use to check for the next token and its attribute.doc[token.i + 1].pos_</br>\n",
    "If a proper noun before a verb is found, print its .token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53b3a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
